Moon Changi

딥러닝을 이용한 object detection에서 SIFT와 HOG는 관련 연구에서 단골로 등장하는 알고리즘이다. 사실 이 이후에도 다양한 feature descriptor가 등장했다. 아래는 예전에(한 6년전?) 작성했던 알고리즘 목록이다.

1. OSID(Ordinal Spatial Intensity Distribution)
Feng Tang, Suk Hwan Lim, Nelson L. Chang, Hai Tao: A novel feature descriptor invariant to complex brightness changes. 2631-2638, CVPR 2009.

2. WLD(Weber Local Descriptor)
Jie Chen; Shiguang Shan; Chu He; Guoying Zhao; Pietikainen, M.; Xilin Chen; Wen Gao; Dept. of Electr. & Inf. Eng., Univ. of Oulu, Oulu, Finland, WLD: A Robust Local Image Descriptor, PAMI 2009.

3. CHoG(Compressed Histogram of Gradients)
V. Chandrasekhar, G. Takacs, D. Chen, S. Tsai, R. Grzeszczuk, and B. Girod, "CHoG: compressed histogram of gradients", IEEE International Conference on Computer Vision and Pattern Recognition, June 2009.

4. BRIEF(Binary Robust Independent Elementary Features)
M. Calonder, V. Lepetit, C. Strecha, and P. Fua, BRIEF: Binary Robust Independent Elementary Features, European Conference on Computer Vision, Heraklion, Greece, 2010.

5. RIFF(Rotation Invariant Fast Features)
G. Takacs, V. Chandrasekhar, S. Tsai, D. Chen, R. Grzeszczuk, and B. Girod, "Unified real-time tracking and recognition with rotation-invariant fast features", IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June 2010.

6. RIFF-Polar((Rotation Invariant Fast Features-Polar)
G. Takacs, V. Chandrasekhar, H. Chen, D. Chen, S. Tsai, R. Grzeszczuk, and B. Girod, "Permutable descriptors for orientation-invariant image matching", SPIE Workshop on Applications of Digital Image Processing (ADIP), August 2010.

7. DAISY
E. Tola, V. Lepetit, and P. Fua. Daisy: An efficient dense descriptor applied to wide-baseline stereo. PAMI, 2010.

8. ORB(Oriented FAST and Rotated BRIEF)
ORB: an efficient alternative to SIFT or SURF, Ethan Rublee (Willow Garage), Vincent Rabaud (willowgarage), Kurt Konolige, Gary Bradski Hiroshi Ishiguro, Osaka University ICCV 2011.

9. DaLI(Deformation and Light Invariant)
Deformation and Illumination Invariant Feature Point Descriptor, Francesc Moreno(Institut de Robotica i Informatica Industrial (UPC/CSIC)), CVPR 2011.



딥러닝 기반 객체 검출 논문들 연도별 목록(2013~2018)

1. 1 단계 기반 검출기
1.1) 앵커 없음
- OverFeat(2013, https://arxiv.org/abs/1312.6229)
- MultiBox(2014, https://arxiv.org/abs/1312.2249)
- DenseBox(2015, https://arxiv.org/abs/1509.04874)
- YOLO(2015, https://arxiv.org/abs/1506.02640)
- UnitBox(2016, https://arxiv.org/abs/1608.01471)
- EAST(2017, https://arxiv.org/abs/1704.03155)
- SFace(2018, https://arxiv.org/abs/1804.06559)

1.2) 앵커 있음
- SSD(2015, https://arxiv.org/abs/1512.02325)
- YOLOv2(=YOLO9000, 2016, https://arxiv.org/abs/1612.08242)
- DSSD(2017, https://arxiv.org/abs/1701.06659)
- RON(2017, https://arxiv.org/abs/1707.01691)
- RetinaNet(2017, https://arxiv.org/abs/1708.02002)
- YOLOv3(2018, https://arxiv.org/abs/1804.02767)

2. 2 단계 기반 검출기
- RCNN(2014, https://arxiv.org/abs/1311.2524v5)
- Fast RCNN(2015, https://arxiv.org/abs/1504.08083)
- Faster RCNN(2015, https://arxiv.org/abs/1506.01497)
- RFCN(2016, https://arxiv.org/abs/1605.06409)
- Light-Head RCNN(2017, https://arxiv.org/abs/1711.07264)
- FPN(2017, https://arxiv.org/abs/1612.03144)
- Mask RCNN(2017, https://arxiv.org/abs/1703.06870)
- RFCN++(2018, https://www.aaai.org/…/index.p…/AAAI/AAAI18/paper/view/16463)
- MegDet(2018, https://arxiv.org/abs/1711.07240)
- DetNet(2018, https://arxiv.org/abs/1804.06215)


객체 인식 시 알고리즘 관점에서 바라본 기본적인 고려 사항에 대해 몇자 적어봅니다.

1. 성질적 관점
- rigid or non-rigid object(involed deformable)?
- 2D or 3D object?
- single or multiple object?
- textured or textureless object?

2. 환경적 관점
- indoor or outdoor scene?
- viewpoint variation(involved rotation,translation), illumination, scale, deformation, occlusion, background clutter, object intra-class variation, local ambiguity, cast or surrounding shadow, etc.

3. 기하학적 관점
- single or multiple view?
- fixed or moving(involved hand-held) view?
- DOF(Degree Of Freedom) estimation.



과거에 쓴 글 회상하기(모빌아이와 암논 샤슈아): 개인적으로 CV 분야에서 가장 성공하며 뛰어난 기업중 하나가 저는 이스라엘의 모빌아이(http://www.mobileye.com/)라는 회사라고 생각을 합니다. 아마도 Vision based Advanced driver assistance systems를 하시는 분들이라면 한번쯤은 다 들어보셨을 것입니다. 제가 모빌아이를 뛰어나다고 생각하는 이유는 아래와 같습니다.

1. 하나의 chip으로 보행자 충돌 경고, 차선 이탈 경고, 전방 추돌 경고, 고속도로 모니터링, 지능형 전조등 제어, 과속 표지판, 사각 지대 인식 등의 기능을 거의 완벽하게(주야/날씨/도로 환경에 상관없이) 수행하고 있기 때문입니다. 참고로 저는 CV 분야를 8년정도 했지만 제가 본 Product 중에 가장 impressive 합니다.

2. 위 1번 사항을 동시에 해결하기 위해서 특화된 자체 센서와 카메라 노출 전략이 들어가 있으며, 알고리즘 또한 엄청나게 강건한 성능을 보입니다. 그러다 보니 글로벌 차량 회사에 Before로 납품이 되겠죠^^

3. 비전에서 상당히 어려운 주제인 outdoor 환경에서의 rigid or non-rigid한 객체들을 동시에 다수 검출 및 추적을 하며, 이를 위해 scene 분석까지 하고 있습니다.

4. 1~3번을 가능하게 한 요인 중 하나는 개인적으로 모빌아이의 창립자인 히브리대의 암논 샤슈아 교수가 있기 때문입니다.(CV 분야의 천재중 하나죠^^)

홈페이지는 아래와 같습니다.
http://www.cs.huji.ac.il/~shashua/research.php

CV 연구자들이 NIPS나 Nature에 논문을 투고하는 경우가 많지 않은 것으로 알고 있는데 암논 샤슈아 교수님은 최근에 NIPS나 Nature에도 투고를 하셨네요. Resume를 보면 상당히 놀라운데, 학교 시절부터 Shimon Ullman 교수와 Tomaso Poggio 교수 등과 같은 대가들로부터 교육을 받았습니다.

사실 개인적으론 Ullman 교수의 제자중에서 가장 CV 분야로 성공한(돈/명예/기술) 사람중 하나가 암논 샤슈아 교수님이 아닐까 생각합니다.

석사 과정 시절에는 이스라엘의 바이츠만연구소(Weizmann Institute of Science)에서 Shimon Ullman 교수(http://www.wisdom.weizmann.ac.il/~shimon/)의 제자로서 공부를 하였고,

박사 과정 때에는 MIT의 인공지능 연구실에서 공부를 하셨습니다.

그리고 포닥 역시 MIT에서의 Tomaso Poggio 교수(http://mcgovern.mit.edu/principal-investigato…/tomaso-poggio) 밑에서 공부를 하셨습니다.

주요 연구 분야는
Multiple View Geometry, Photometric Issues in Visual Recognition, Statistical Issues in Visual Recognition, Algebraic Systems for Image Coding and Statistical Inference 등이 있고

홈페이지를 보시면 아시겠지만, 수상 경력이나 학회 활동 전문 기술이원 등의 활동 역시 상당한 것으로 알고 있습니다.

ADAS 분야를 하시는 분들에게 좋은 정보가 되셨길 바랍니다.



페친분의 요청으로 다시 올리는 글입니다. 박세진 이거 말씀하신거 맞죠?

MS의 Rick Szeliski 박사님과 옥스포드의 Andrew Zisserman 교수님이 이야기하는 "모든 컴퓨터 비젼 연구자들이 알아야 할 20개의 techniques" 입니다.

1. Image formation and optics

2. Image processing, filtering, Fourier analysis

3. Pyramids and wavelets

4. Feature extraction

5. Image matching

6. Bag of words

7. Optical flow

8. Structure from motion

9. Multi view stereo

10.Segmentation

11.Clustering

12.Viola-Jones

13.Bayesian techniques

14.Machine learning

15.RANSAC and robust techniques

16.Numerical methods

17.Optimization

18.Range finding, active illumination

19.Algorithms

20.Graph cuts

21.Dynamic programming

22.Complexity analysis

23.MATLAB and C++. and assembly (optional: GPU programming)

24.Communication and presentation skills

Image and features

• NCC

• Interest point operators

• Scale invariant and affine invariant detectors & descriptors

• Scale space

• Image processing, filtering, Fourier analysis

• Pyramids and wavelets

• Edge detection

• Restoration e.g. deblurring, super-resolution

– Linear, e.g. Wiener filter

– MRF

– Non-local means/BM3D/bilateral filter

Segmentation, grouping and tracking

• Segmentation

– Normalized cuts

• Grouping

– Hough transforms

• Clustering

– K-means

– Mean-shift

– Pedro-clustering

• Tracking

– Kalmanfilter

– Particle filter

Multi-view: stereo, SFM, flow

• RANSAC and other robust techniques

• Geometry:

– epipolar geometry (projective and affine)

– planar homographies

– Affine camera

• Geometry estimators

– 8 point algorithm for F

– 4 point algorithm for H

• Factorization

• Bundle-adjustment

• Flow

– Horn & Schunck L2

– Lucas-Kanade

– L1 regularized

Recognition

• Bag of visual words

• HOG, SIFT, GIST

• Spatial pyramid

• Spatial configurations/Pictorial structures

• Sliding window/jumping window

• Cascades

Others

Machine Learning

– Adaboost

– kNN

– SVM

– Random forest

– PCA, ICA, CCA

– EM

– MIL/Latent-SVM

– Regularization

– HMM

– Graphical & Bayesian models

Optimization

– Classical linear and non-linear

– Graph operations

– Dynamic programming/message passing for MAP, max-marginals

– Graph cuts for binary variable MAP

• Texture synthesis


컴퓨터 비젼 알고리즘을 공부하려고 하는 초심자분들을 위해 제가 공부하는(했던) 방법에 대해서 공유를 해드릴까 합니다. 제 방법이니 정답은 아니고 참고하시면 좋을 것 같습니다.

저는 알고리즘 구현은 라이브러리를 사용하는 것도 중요하지만, 원리부터 이해하고 가능한 밑바닥부터 논문보고 구현하는 것을 선호하는 편입니다. 어차피 라이브러리가 없으면 직접 구현을 해야하고, 혹은 있어도 타겟 플랫폼이 지원하지 않는다면 사용이 불가능한 경우가 많이 있기 때문이죠(경험상 회사에서도 이렇게 알고리즘 개발 자유도가 높은 사람을 훨씬 더 선호하는 것 같습니다).

어쩌면 라이브러리를 쓸 수 없는 환경들에서 개발을 해왔던 탓도 있고, 원래 밑바닥부터 하는 것을 좋아하는 성향도 있는 것 같기 때문에 그런 선호가 생긴 것 같습니다. 그럼 저의 방법에 대해서 예를 들어 이야기를 드리겠습니다.

영상처리 알고리즘 중에서 Otsu 라는 이진화 알고리즘이 있습니다. https://engineering.purdue.edu/…/c…/ECE661.08/OTSU_paper.pdf 1979년도에 나왔는데도 29000 회 이상의 인용 지수를 가지며, ICDAR 과 같은 컨테스트들에 나오는 페이퍼에서도 아직도 참고 문헌으로 언급되는 유명하는 논문이죠.

논문을 읽고, 구현을 할 때 저의 경우는 컴파일 언어 하나와 스크립트 언어 하나씩 구현을 하는 편입니다. 적어도 컴파일 언어 하나와 스크립트 언어 하나는 꼭 수련하는 것이 좋다고 생각해서, 이를 위한 적합한 언어로 저는 c(or c++)와 python을 씁니다.

먼저 제가 가장 자신 있는 c(or c++) 언어로 구현한 후 python으로 구현을 합니다. 그리고 해당 알고리즘이 라이브러리가 있다면 구현 결과가 맞는지 확인을 하고 더불어 속도를 체크합니다. 결과가 같고 타겟 플랫폼이 지원을 한다면 가장 빠른 속도를 보이는 구현 언어를 택하는게 좋겠죠.

사실 뭐 할 줄 아는 언어가 이 2개밖에 없기도 하고 요즘 딥러닝 프레임워크들은 python 기반이 워낙 잘 되어 있어서 필요한 비전 알고리즘을 python으로 구현해놓으면 붙이기에 용이하더군요. c언어는 타겟 플랫폼이 저사양의 임베디드인 경우도 있기 때문에 포팅을 위해서 구현을 하는 편입니다.

참고로 아래 댓글에 Otsu 알고리즘을 c언어와 python으로 구현한 코드의 일부분을 찍어서 올립니다. 제 공부 방법이 정답은 아니지만, 초심자분들에게 도움이 되라는 취지로 올리니 부디 유익한 정보가 되셨으면 합니다.



다소 오래되긴 했지만 컴퓨터 비전 한다고 하면 꼭 필요한 바이블 서적들.

*영상처리:
1.Digital Image Processing (3rd Edition)
https://www.amazon.com/Digital-Image-Processing-Rafael-Gonzalez/dp/013168728X/ref=sr_1_1?s=books&ie=UTF8&qid=1322456619&sr=1-1&fbclid=IwAR2CJPrsihgLgXH-Gxg_zx4ZrhUy3FpMZpU64gX9ZJ7SHLYUGQkz205L7ps

*컴퓨터비전:
1. Computer Vision: Algorithms and Applications
https://www.amazon.com/Computer-Vision-Algorithms-Applications-Science/dp/1848829345/ref=sr_1_1?s=books&ie=UTF8&qid=1322456678&sr=1-1&fbclid=IwAR1l0JPYVagJa6zejbNXv80lG87v-4WXz3oAeq4oPpkQ-AfVc5axDvJXiSQ

2. Computer Vision
https://www.amazon.com/Computer-Vision-Linda-G-Shapiro/dp/0130307963/ref=sr_1_3?s=books&ie=UTF8&qid=1322456678&sr=1-3&fbclid=IwAR3LJQmHfLNCd_g8kD1iYTc8bOgicAeXxEFimycaKV74Zun3Ke8D-NpjecY

*패턴인식:
1. Pattern Recognition and Machine Learning
https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?s=books&ie=UTF8&qid=1322456771&sr=1-1&fbclid=IwAR07wFmx-ZkQLUxn82nls3wJvxmfqDj_JmzHr6KM2cku8hom00tFmaPJZmY

*기하학:
1. Multiple View Geometry in Computer Vision
https://www.amazon.com/Multiple-View-Geometry-Computer-Vision/dp/0521540518/ref=sr_1_1?s=books&ie=UTF8&qid=1322456815&sr=1-1&fbclid=IwAR35iS27tDBix4dMbCaC1sqk-cPu1lj8D2WhzkIJhK_fX9lY_n8YRnwgbYg



자신이 고안한 노이즈 제거 알고리즘이나 컨트라스트 향상 알고리즘이 있는 경우 과연 얼마나 기존에 비해서 개선이 되었냐를 평가할 때 가장 많이 알려진 방법은 Mean Squared Error와 같이 local error의 difference를 sum하는 방식으로 구현되는 peak
signal-to-noise ratio(PSNR)이 대표적입니다. 하지만 이렇게 error에 대한 sensitivity를 기반으로 하는 유사도 측정 방식은 다양한 한계점들을 가진다는 것이 많은 연구자들을 통해서 증명이 되어왔으며, 하기의 논문은 이러한 문제를 극복하기 위해 이론적인 고찰을 통해 Structural SIMilarity(SSIM)라고 불리우는 나름대로의 metric를 만들고 접근하고 있습니다. 참고로 원래 논문과 멀티 스케일 버전 논문을 둘다 구현해서 평가해본 결과 어느 정도 신뢰할만한 결과를 보이더군요.

원래 논문: Image Quality Assessment: From Error Visibility to Structural Similarity(IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 4, APRIL 2004)
링크: https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf

멀티 스케일을 고려한 논문: Analyzing the Role of Visual Structure in the Recognition of Natural Image Content with Multi-Scale SSIM(SPIE Human Vision and Electronic Imaging XIII, San Jose, CA 2008)
링크: http://foulard.ece.cornell.edu/publi…/dmr_hvei2008_paper.pdf

원 논문은 인용지수가 8000회가 넘을 정도로 많이 알려진 유명한 논문인데, 우리나라 비전 관련 연구자들은 생각외로 많이 모르시는 것 같아서 정보 공유 차원에서 올려봅니다. 제 연구 분야는 아니지만 관련 분야를 연구하시는 분들에게 도움이 되시길 바랍니다.





8년전에 쓴 글:
컴퓨터 비전을 이용한 다양한 응용 분야에 있어서, 이제는 global feature는 표현의 위험성과 부정확성으로 인해서 많이 사용되고 있지 않으며, local feature를 이용한 연구가 어느 순간부터 주를 이루는 것 같다. 개인적으로 생각하건데, 이는 "인식"이라는 주제가 비전 기술의 상용화를 바라는 많은 연구자들에게 관심을 받기 시작하면서 관련 연구에 대해 싸움을 시작한 듯 싶다. 컴퓨터 비전 분야의 다양한 연구 주제중에서 local feature가 활발하게 연구되고 적용되는 주제를 살펴본 결과 image retrieval, image registration, object recognition, object categorization, texture classification, robot localization, wide baseline matching, video shot retrieval 등이 있다. 대분류가 저러하니, 세부적으로 들어가면 또한 상당히 많은 연구들이 존재할 것이다.

위와 같은 주제에서 local feature를 이용하는 방법은 크게 전통적인 기법과 이에 clustering의 개념과 image represensation의 개념을 도입한 기타 방법이 존재한다. 전통적인 기법은 크게 3가지 단계로 구성이 되며, 각 단계는 feature detection, feature description, feature matching 등으로 이루어진다. 반면에 기타 방법에서는 feature detection, feature description, feature clustering, frequcency histogram construction(for image representation) 등의 4가지 단계로 구성이 된다. 전통적인 기법에서는 유사도를 측정할 때 분류 개념을 도입하지 않고 바로 정합을 이용하며, 기타의 기법에서는 유사도를 측정할 때 정합이 아닌 분류의 개념을 도입하여 이용하게 된다. 전통적 기법에서의 대표적 연구로는 우리에게 친숙한 SIFT(2004, Lowe)나 SURF(2006, Bay)등이 있으며, 기타 기법에서의 대표적 연구로는 BOF(Bag Of Features, 2006, Nowak)나 Hyperfeatures(2005, Agarwal) 등이 있다.

잘 알다시피 컴퓨터 비전 분야에서의 local feature는 크게 두가지로 구성이 된다. 첫번째는 feature detector 이며 그 나머지는 feature descriptor 이다. feature detector는 feature가 표현하는 대상이 point인지 region인지에 따라서 point를 이용한 접근 방법과 region을 이용한 접근 방법으로 나뉜다. feature descriptor는 크게 feature를 어떠한 방법으로 description할지에 따라서 filter를 기반으로 한 접근, distribution을 기반으로 한 접근, texture를 이용한 접근, derivative를 기반으로 한 접근 그리고 기타 방법인 moment를 기반으로 한 접근, phase를 기반으로 한 접근, color를 기반으로 한 접근 등으로 나뉜다. 각 분류에 따른 상세한 내용은 추후에 언급을 할 예정이다. 한가지 분명한 사실은 feature를 detection하는 연구뿐 아니라 얼마나 효율적으로 description할 수 있는지에 대한 연구 역시 활발하게 진행되고 있다는 사실이다. 결국 detection된 정보량을 얼마나 손실없이 혹은 강건하게 description할 수 있는지가 관련 연구의 핵심인 듯 싶다.

우선적으로 살펴볼 내용은 feature detector에 관한 것이다. 그 중에서도 point를 기반으로 한 접근부터 살펴보도록 하자. 사실 이는 1977년 Moravec의 연구에 기원을 하고 있다. Moravec은 최소한의 밝기값의 변화를 가지는 local maximum을 찾으려고 노력했으며 이를 반영한 새로운 corner detector를 제안한다. 추후 연구로 1988년에 Harris와 Stephens은 Moravec의 연구에서 제안한 cornder detector의 response는 anisotropic하며, noisy하고 edge에 sensitive한 사실을 발견하게 되었고 이를 극복하기 위해서 우리에게 친숙한 Harris corner detector를 제안하게 된다. 개인적으로 edge detector에 있어서 canny가 꽤나 진보한 방법을 제안했듯이 corner detector에 있어서는 harris가 꽤나 진보된 방법을 제안했다고 생각을 한다. 보다 robust한 interest point를 찾기 위한 대상의 feature가 edge에서 corner로 넘어간 시점인듯 싶기도 하다(물론 오늘날에도 edge라는 feature는 상당히 중요한 역할을 한다). 이렇게 제안된 Harris Corner Detector는 컴퓨터 비전의 다양한 주제에 있어서 한동안 꽤나 충실한 corner detector의 역할을 하지만 Scale 변화에 약하다는 점을 지니고 있었다. 그런데 이 약함의 정도는 상당히 취약한 것이었다(사실 Harris의 방법을 물리적으로 분석해보면 Scale 변화에 대한 고려는 거의 하고 있지 않다). 아마도 이 시점에서 많은 비전 연구자들의 관심은 Scale 변화에 강건한 방법을 찾기 위한 노력으로 바뀌기 시작했을 거라고 추측이 된다. 개인적으로는 기존의 point에 대한 접근이 아닌 region에 대한 접근으로의 전이가 발생했던 시점이라고 생각한다. 하지만, SUSAN(1997), FAST(2006) 등과 같이 큰 scale의 변화는 무시하고 작은 scale의 변화에 대해서 강건하며 속도를 빠르게 하는 방법으로의 연구는 계속되었으며 특히 최근에 모바일 환경에서의 CV 기술의 필요는 이러한 연구를 다시 활성화 시키고 있다. 모바일 환경에서 CV 알고리즘 구현 시 런타임 때의 메모리 사용과 CPU 점유율이 이슈이기 때문에 이를 개선하기 위해 많은 연구들이 진행중이다.

어찌되었거나 다시 Scale 변화에 대한 내용으로 돌아와서, 이러한 노력의 선봉장이었던(과거 Lowe 교수의 연구를 통해 살펴본 개인적 견해임) Lowe 교수는 2004년도에 SIFT라는 획기적인 방법을 제안하게 된다. 제목에서도 Scale에 invariant함을 표시했으니 그가 얼마나 이에 대해 갈망하고 있었는지 짐작할 수 있을 것 같다. 여기서 잠깐 생각해 볼 것이 있는데 Scale라는 요소는 컴퓨터 비전 분야에서 geometirc한 변환에 해당이 되며 rotation, translation, viewpoint 등이 추가로 해당이 된다. 참고로 photometric한 변환으로는 blur, illumination, noise, JPEG compression 등이 있다. 중요한 것은 최근의 이 분야의 연구 방향은 이렇게 다양한 geometric 변환과 photometric 변환에 대해 각각의 강건함을 최대한 유지시키기 위한 노력으로 연구되고 있다는 사실이다. 잠시 이야기가 다른 방향으로 흘렀다. 다시 SIFT로 돌아와서 살펴보도록 하자.

SIFT는 크게 detector와 descriptor로 구성이 된다. 물론 추후에 Bay 교수에 의해서 나온 SURF도 이와 마찬가지다. 그래서 혹자들은 SURF를 SIFT의 변종이나 확장의 개념으로 언급하기도 하나 개인적으로 이러한 견해는 반대이다. 큰 맥락은 비슷하지만 내부적으로 살펴보면 SURF에서는 SIFT의 단점을 분석하여(특히나 속도면) 새로운 아이디어를 많이 넣었기 때문이다. 이에 대해서는 추후에 다시 살펴보도록 하겠다. SIFT의 detector는 scale space의 octave 및 이들 내부의 layer에서 extrema를 찾기 위한 방법으로 연속적인 DoG(Differenc of Gaussian) 영상에서 local maximum을 찾도록 접근한다.

Lowe 교수의 SIFT 연구 후 같은 년도에 Mikolajczyk와 Schmid는 Harris-Laplace Detector를 제안하게 된다. 이 방법은 Harris corner detector의 방법과 특징적인(characteristic) scale를 찾기 위한 laplacian function을 결합시켜 제안된 방법이다. 취지는 scale에 invaraint한 feature를 detection하기 위함이었다. 또한 이들은 view point의 변화에 대한 문제를 해결하고자, Harris(Hessian) affine detector를 적용하게 되는데 이는 Harris corner detector(Hessian point detector), scale selection, 타원 모양의 추정에 기반을 한 second moment matrix 등의 구성으로 이뤄지게 된다. 동일한 년도에 Tuytelarrs와 Van Gool은 edge를 기반으로 하는 detector를 제안하게 되는데, 이는 Harris corner points와 연관된 평행사변형의 모양을 만들기 위해서 curve 모양의 에지와 straight 모양의 에지를 모두 고려하는 접근이다. 또한 이들은 같은 논문에서 intensity를 기반으로 하는 detector를 제안하게 되는데, 이는 밝기값의 local extrema로 시작하여 extrema로부터 산출된 다양한 선들로 이뤄진 타원형의 영역을 구성하게 되고 이 영역 상에서 detection을 하는 방법을 취한다. 즉 edge와 intenity를 기반으로 한 접근은 affine에 불변하는 region을 최대한 찾기 위한 시도라고 볼 수 있다. 또한 동일한 년도에 Kadir과 Zisserman은 entropy fuction을 이용하여 salient한 region을 찾기 위한 detector를 제안하게 된다. SIFT가 나왔던 2004년도에만 이렇게 관련 분야에서 다양한 논문들이 발표되었으니 SIFT의 파급효과는 상당했던 것으로 짐작이 된다. 사실 이러한 연구가 발표되기 이전인 2002년도에 Matas 교수는 MSER(Maximally Stable Extremal Region)을 제안하고 이를 stereo matching에 적용하여 우수한 성능을 보였다. MSER의 경우는 영상 분할을 이용한 접근으로서 워터쉐드 알고리즘과 비슷하며 scale space를 만들지 않으므로 속도면에서 빠르다는 장점이 존재한다. 추후에 MSER의 성능이 입증되면서 이를 이용한 관련 연구도 꽤나 활발하게 연구가 된다.

2006년도에는 SIFT와 큰 맥락에서의 메커니즘은 비슷하지만 속도를 약 3배정도 빠르게 한 SURF라는 알고리즘이 Bay 교수에 의해서 탄생하게 된다. SURF의 경우는 feature detection에 있어서 integral image를 이용하고 box filter라 불리우는 gaussian filter의 approximation을 이용하여 scale space 상에서 convolution 복잡도와 시간을 크게 줄이게 된다. 물론 feature descriptor 생성에 있어서도 SIFT가 128차원을 만드는 반면에 SURF는 64차원을 만들고(물론 디스크립터의 차원은 조절이 가능하다) laplacian의 sign을 이용하여 미리 정합대상이 되는 특징점들을 indexing함으로서 속도를 빠르게 한다(SURF가 나올 당시에는 SIFT만하겠어 하는 생각이었지만 개인적 실험에 의하면 정확성은 SIFT보다 아주 미흡하게 떨어질지 모르나, 극복해야할 문제인 속도에 대한 측면은 확실히 빨라졌으므로 꽤나 대단한 방법이라고 생각을 한다).

자 그러면 위의 다양한 방법들을 이용하여 검출된 point와 region를 어떻게 효율적으로 표현할 수 있는가에 대한 연구에 대해서 살펴보도록 한다. 즉 이를 위해서 엄청나게 많은 local descriptor가 탄생하게 된다. 이에 대한 가장 초기 연구는 local derivatives를 이용하여 descriptor를 만드는 방법으로 1987년도에 Kornfrtink가 제안했던 방법이다. 생각해보니 point와 region을 detection하기 위한 연구의 역사만큼이나 descriptor의 생성에 대한 연구도 꽤나 오래되었다는 것을 알 수 있다.

--- 추후 업데이트 예정 ---

wrote by CAPTIG(Computer vision And Pattern recognition Technical Interest Group)




단상: 음성 인식 입문자가 적어도 알아야 할 기본적인 지식들(생각나는대로 적어봄)
1. 음향학, 음운론
2. mfcc를 이용한 특징 추출
3. gmm/hmm을 이용한 음향 모델링(고전적 기법)
4. dnn/hmm을 이용한 음향 모델링(딥러닝 기법)
5. N-gram을 이용한 언어 모델링
6. 어휘 사전 구축의 개념
7. WFST를 이용한 디코딩

위의 이해가 선행되면 그 다음엔 e2e 학습에 대해서 공부









Sung Kim

딥러닝 알고리즘들을 가장 빠르게 이해하는 방법중 하나는 이를 직접 구현해 보는 것일텐데요. 어떤 논문을 구현하는 것이 좋을까요? 저도 궁금했던 질문인데 마침 레딧에서 열띤 토론이 벌어지고 있습니다.

https://www.reddit.com/…/d_what_deep_learning_papers_shoul…/

그중 kthx0 님이 정리해주신 리스트는 매우 좋네요. 구현 고고!

Architectures
AlexNet: https://papers.nips.cc/…/4824-imagenet-classification-with-…

ZFNet: https://arxiv.org/abs/1311.2901

VGG16: https://arxiv.org/abs/1505.06798

ResNet: https://arxiv.org/abs/1704.06904

GoogLeNet: https://arxiv.org/abs/1409.4842

Inception: https://arxiv.org/abs/1512.00567

Xception: https://arxiv.org/abs/1610.02357

MobileNet: https://arxiv.org/abs/1704.04861

Semantic Segmentation
FCN: https://arxiv.org/abs/1411.4038

SegNet: https://arxiv.org/abs/1511.00561

UNet: https://arxiv.org/abs/1505.04597

PSPNet: https://arxiv.org/abs/1612.01105

DeepLab: https://arxiv.org/abs/1606.00915

ICNet: https://arxiv.org/abs/1704.08545

ENet: https://arxiv.org/abs/1606.02147

Generative adversarial networks
GAN: https://arxiv.org/abs/1406.2661

DCGAN: https://arxiv.org/abs/1511.06434

WGAN: https://arxiv.org/abs/1701.07875

Pix2Pix: https://arxiv.org/abs/1611.07004

CycleGAN: https://arxiv.org/abs/1703.10593

Object detection
RCNN: https://arxiv.org/abs/1311.2524

Fast-RCNN: https://arxiv.org/abs/1504.08083

Faster-RCNN: https://arxiv.org/abs/1506.01497

SSD: https://arxiv.org/abs/1512.02325

YOLO: https://arxiv.org/abs/1506.02640

YOLO9000: https://arxiv.org/abs/1612.08242




