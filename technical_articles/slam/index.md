# SLAM

2020년 3월 10일

- (익명) MRPT란?스페인 교수가 본인 로봇 제어를 위해 만든 것 같습니다. 로컬라이제이션만 따져 보면 ROS의 AMCL에 비해 구현을 잘 해 놨어요.

- (신ㄷㅇ) 단안 카메라로 SLAM을 돌려 볼 수 있는 예제는 [pyslam](https://github.com/luigifreda/pyslam)이 있습니다.
- (이ㅈㅎ) LiDAR 캘리브레이션은 매트릭스 하나로 보정하는게 쉽지 않습니다. 센서 구조나 부품에 따라 차이가 있으므로, 핀홀 카메라로 가정하기 어렵습니다. 제조사에서 보정을 하여 출고 합니다.
- (장ㅎㄱ) 자율주행쪽에서는 Visual-SLAM만 사용하지는 않을 것입니다. ORB-SLAM도 사용하지 않을 것이고, 사용하더라도 튜닝을 하였을 것이다. 보통 sensor fusion을 하여 uncertainty를 줄이는 방향으로 사용 합니다.  보통 Lidar SLAM이 Visual SLAM보다 정확도가 높은 편인데, Lidar SLAM의 정확도가 낮아질 때 Visual SLAM의 정보와 혼합함으로써 ego-motion과 map에 대한 정보에 대한 uncertainty를 줄일 수 있지 않나 생각합니다

- (이ㅈㅎ) 자율주행도 차인지 로봇인지 실내인지 실외인지에 따라 다 다른데, 자율주행차를 기준으로 말씀하시는거면 HD map이 있다는 가정하에 SLAM 기반 Localization을 할때는 이미지기반 visual slam보다는 오히려 lidar 기반 slam이 신뢰도도 높고 더 많이 쓰입니다. 

  그리고 센서 융합은 크게는 두가지로 나눈다고하면 loosely-coupled와 tightly-coupled로 나눌 수 있을 것 같은데요. 

  쉽게 이야기하면 서로 다른 센서들로부터 나온 데이터들을 하나의 세트로 생각하고 최적화를 하거나, 각각의 결과물들을 이용해 서로 보정하는 방식으로 최적화를 해나가거나 하는 경우가 일반적입니다.

  설명하는 순서가 크로스 되어 있긴한데, 여튼 가능하다면 성능면에서는 잘만 구성할 수 있다면 당연히 같이 사용하는 것이 이점이 많습니다.

  장ㅎㄱ님께서 말씀하신 부분이 uncertainty 이슈도 있고 redundancy 이슈 이기도한데 특정 센서들이 고장나거나 성능이 떨어졌을때를 대비해서 시스템 안정성을 높이기 위해 성능과는 별개로 센서퓨젼을 하기도 합니다.

- (강ㅅㅎ) Visual odometry+loop closure = orb slam 이니까 Orb slam에서 odom 토픽만뽑아서 ekf의 3D odom 에넣주고 2d lidar 의 odom과 ekf 하면 odom localization이 깨지는 문제 보완이 될까요?

- 실내도 큰틀에서는 비슷한데 3d lidar 까지는 사용안하시는 경우가 많은 것 같습니다.

  실내에서 2d lidar만 사용하는 경우는 사실 grid 기반이나 다른 더 쉬운 방법도 많이 있어서 시스템에 맞게 찾아보시면 좋을 것 같습니다. 그리고 imu 사용도 잘 고려해보시면 좋을 것 같습니다. 특히, 모바일이면 사용을 권장드립니다.



## 참조

- 저희는 SLAM_마스터가 될것입니다 카카오톡 단톡방