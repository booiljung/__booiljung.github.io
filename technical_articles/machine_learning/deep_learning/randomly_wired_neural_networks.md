# Exploring Randomly Wired Neural Networks for Image Recognition에 대한 이해

2019년 4월 7일

이 글은 Facebook AI Research (FAIR)의 Saining Xie, Alexandar Kirillov, Ross Girshick, Kaiming He의 논문 Exploring Randomly Wired Neural Networks for Image Recognition를 이해하는 과정에서 기록한 글입니다. [논문 링크](https://arxiv.org/abs/1904.01569)

## Abstract

Neural  networks  for  image  recognition  have  evolved through  extensive  manual  design  from  simple  chain-like models to structures with multiple wiring paths.  The success  of  ResNets  [11]  and  DenseNets  [16]  is  due  in  largepart to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types,  however,  the space ofpossible wirings is constrained and still driven by manual design despite being searched.  In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process.  Encapsulation provides a unified view of NAS and randomly wired networks.  Then, we use three classical random graph models to generate randomly wired graphs for networks.  The results are surprising: several variants of these random generators  yield  network  instances  that  have  competitive  accuracy on the ImageNet benchmark.  These results suggestthat new efforts focusing on designing better network generators  may  lead  to  new  breakthroughs  by  exploring  less constrained search spaces with more room for novel design.

## 1. Introduction
What  we  call  deep  learning  today  descends  from  the connectionist approach  to  cognitive  science  [38,  7]— a paradigm reflecting the hypothesis that how computational networks  are  wiredis  crucial  for  building  intelligent  machines.  Echoing this perspective, recent advances in computer vision have been driven by moving from models with chain-like wiring [19, 53, 42, 43] to more elaborate connectivity patterns, e.g., ResNet [11] and DenseNet [16], that are effective in large part because of how they are wired. 

Advancing this trend, neural architecture search (NAS)[55,  56]  has  emerged as  a  promising  direction  for  jointly searching  wiring  patterns  and  which  operations  to  perform.  NAS methods focus onsearch [55, 56, 33, 26, 29,27]  while implicitly  relying on  an important — yet  largely overlooked— component that we call a network generator (defined  in  §3.1).   The  NAS  network  generator  defines  a family  of  possible  wiring  patterns  from  which  networks are sampled subject to a learnable probability distribution. However, like the wiring patterns in ResNet and DenseNet, the NAS network generator ishand designed and the spaceof allowed wiring patterns is constrained in a small subset of all possible graphs. Given this perspective, we ask: What happens if we loosen this constraint and design novel network generators?

We explore this question through the lens of randomly wired  neural  networks that  are  sampled  from  stochastic network  generators,  in  which  a  human-designed random process defines  generation.   To  reduce  bias  from  us — the authors of this paper — on the generators, we use three clas-ical families of random graph models in graph theory [51] : the  Erd ̋os-R ́enyi  (ER)  [6],  Barab ́asi-Albert  (BA)  [1],  and Watts-Strogatz (WS) [50] models.  To define complete networks, we convert a random graph into a directed acyclic graph (DAG) and apply a simple mapping from nodes to their functional roles (e.g., to the same type of convolution).

The results are surprising:  several variants of these random generators yield networks with competitive accuracyon  ImageNet  [39]. The  best  generators,  which  use  the WS model, produce multiple networks that outperform orare  comparable  to  their  fully  manually  designed  counter-parts and the networks found by various neural architecture search methods.  We also observe that the variance of accuracy is low for different random networks produced bythe same generator, yet there can be clear accuracy gaps between different generators. These observations suggest thatthe network generator designis important. 

We  note  that  these  randomly  wired  networks  are not “prior  free”  even  though  they  are  random.   Many  strong priors are in fact implicitly designed into the generator, including  the  choice  of  a  particular  rule  and  distribution  to control the probability of wiring or not wiring certain nodes together.  Each random graph model [6, 50, 1] has certain probabilistic behaviors such that sampled graphs likely exhibit certain properties (e.g., WS is highly clustered [50]). Ultimately, the generator design determines a probabilistic distribution over networks, and as a result these networks tend to have certain properties. The generator design underlies the prior and thus should not be overlooked.

Our work explores a direction orthogonal to concurrent work on random search for NAS [23, 41].   These studies show that random search is competitive in “the NAS searchspace” [55, 56], i.e., the “NAS network generator” in ourperspective.   Their  results  can  be  understood  as  showingthat the prior induced by the NAS generator design tendsto  produce  good  models,  similar  to  our  observations. Incontrast  to  [23,  41],  our  work  goes  beyond  the  design  of established NAS generators and explores different random generator designs.

Finally, our work suggests a new transition from designing  an  individual  network  todesigning  a  network  generator may  be  possible,  analogous  to  how  our  community have  transitioned  from  designing  features  to  designing  a network that learns features.  Rather than focusing primarily on search with a fixed generator, we suggest designing new network generators that produce new families of models for searching.  The importance of the designed network generator (in NAS and else where) also implies that machine learning has not been automated (c.f. “AutoML” [20])—the underlying human design and prior shift from network engineering to network generator engineering.

## 2. Related Wrok

**Network wiring.** Early recurrent and convolutional neural networks (RNNs and CNNs) [37, 21] use chain-like wiring patterns. LSTMs  [14]  use  more  sophisticated  wiring  to create a gating mechanism.   Inception CNNs [45, 46, 44] concatenate multiple, irregular branching pathways, while ResNets [11] use $x + \mathcal F(x)$ as a regular wiring template; DenseNets [16] use concatenation instead: $[x, \mathcal F(x)]$.  The LSTM,  Inception,  ResNet,  and  DenseNet  wiring  patternsare effective in general, beyond any individual instantiation.