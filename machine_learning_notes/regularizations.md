[Up](index.md)

# Regularization

## Batch Normalization

* Batch Normalization #1 - 라온피플 [blog](https://laonple.blog.me/220808903260)
* Batch Normalization #2 - 라온피플 [blog](https://laonple.blog.me/220811172205)
* Batch Normalization - SanghyukChun [blog](http://sanghyukchun.github.io/88/)
* Batch Normalization 설명 및 구현 - Beomsu Kim [blog](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)
* Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - 2015 [paper](https://arxiv.org/abs/1502.03167)

## Dropout

* A Simple Way to Prevent Neural Networks from Overfitting - 2014 [paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout)
* The dropout learning algorithm [paper](https://www.sciencedirect.com/science/article/pii/S0004370214000216)
* Dropout #1 - 라온피플 [blog #1](https://laonple.blog.me/220818841217)
* Dropout #2 - 라온피플 [blog #2](https://laonple.blog.me/220823177178)
* Dropout #2 - 라온피플 [blog #3](https://laonple.blog.me/220827359158)

### DropConnect

* Regularization of Neural Networks using DropConnect [paper](https://cs.nyu.edu/~wanli/dropc/dropc.pdf)
* DropConnect - 라온피플 [blog](https://laonple.blog.me/220827359158)

## Stochastic Pooling

* Stochastic Pooling for Regularization of Deep Convolutional Neural Networks - 2013 [paper](https://arxiv.org/abs/1301.3557)
* Stochastic Pooling - 라온피플 [blog #1](https://laonple.blog.me/220830178487)

## Maxout Networks

* Maxout Networks - 2013 [paper](https://arxiv.org/abs/1302.4389)
* From Maxout to Channel-Out: Encoding Information on Sparse Pathways [paper](https://arxiv.org/abs/1312.1909)
* Maxout Networks - 라온피플 [blog #1](https://laonple.blog.me/220836305907)
